# WebScraper
Парсер с habr.com:

1) Парсер раз в 10 минут делает запрос на главную страницу хаба.
2) Берет с главной страницы хаба ссылки на статьи.
3) Для каждой собранной ссылки посещает страницу статьи и собрает информацию о статье (заголовок, дата, ссылка на пост, имя автор, ссылка на автора).
4) Выводит информацию на консоль

А также:

LVL1: Сохраняет данные в базу данных sqlite3 с текстом публикации. 

LVL2: Создает таблицу в базе данных с информацией о хабах.
      Добавляет в созданную таблицу все хабы.

LVL3: Парсер асинхронный, используется библиотека aiohttp

LVL4: Добавлена админка на Django для отображения хабов и управления ими (можно добавить хаб/удалить хаб).

## Как запустить проект:

Клонировать репозиторий:

```
git clone git@github.com:inovaras/WebScraper.git
```

Перейти в него в командной строке:
```
cd WebScraper
```

Cоздать и активировать виртуальное окружение:

```
python -m venv env
```

```
source venv/Scripts/activate
```

```
python -m pip install --upgrade pip
```

Установить зависимости из файла requirements.txt:

```
pip install -r requirements.txt
```

Выполнить миграции:

```
python manage.py migrate
```

Запустить проект:

```
python manage.py runserver
```
Админка http://127.0.0.1:8000/admin/parser/article/


## Заполнение базы данных:
Скрипт парсера https://habr.com/ru и загрузки данных в бд находится в 
```web_scraper > parser > management > commands > async_parse_hub_and_fill_bd``` 

## Запуск скрипта по команде:
```
python manage.py async_parse_hub_and_fill_bd
```
